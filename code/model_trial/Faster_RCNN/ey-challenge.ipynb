{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhK7Iq2jz8Xn1ZHAaFnFxb"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ellenguyen/CIS4496_EY.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PV4mOP0vQ5zx","executionInfo":{"status":"ok","timestamp":1712027431516,"user_tz":240,"elapsed":1183,"user":{"displayName":"Le Mary","userId":"04873967065408965869"}},"outputId":"a7db59bc-f82c-4595-b9e1-9dcd64b171d9","execution":{"iopub.status.busy":"2024-04-02T05:09:05.361405Z","iopub.execute_input":"2024-04-02T05:09:05.362206Z","iopub.status.idle":"2024-04-02T05:09:06.325022Z","shell.execute_reply.started":"2024-04-02T05:09:05.362173Z","shell.execute_reply":"2024-04-02T05:09:06.323998Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'CIS4496_EY' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/CIS4496_EY/given/AugmentedData')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T05:09:07.997137Z","iopub.execute_input":"2024-04-02T05:09:07.997529Z","iopub.status.idle":"2024-04-02T05:09:08.002792Z","shell.execute_reply.started":"2024-04-02T05:09:07.997493Z","shell.execute_reply":"2024-04-02T05:09:08.001662Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport shutil","metadata":{"id":"5SL9MPB5W-IZ","execution":{"iopub.status.busy":"2024-04-02T05:09:10.545719Z","iopub.execute_input":"2024-04-02T05:09:10.546057Z","iopub.status.idle":"2024-04-02T05:09:11.099645Z","shell.execute_reply.started":"2024-04-02T05:09:10.546031Z","shell.execute_reply":"2024-04-02T05:09:11.098904Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision import transforms\nimport os\nfrom PIL import Image\nimport numpy as np","metadata":{"id":"nErhFVaxVS1C","execution":{"iopub.status.busy":"2024-04-02T05:09:13.617629Z","iopub.execute_input":"2024-04-02T05:09:13.618587Z","iopub.status.idle":"2024-04-02T05:09:16.980357Z","shell.execute_reply.started":"2024-04-02T05:09:13.618553Z","shell.execute_reply":"2024-04-02T05:09:16.979307Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        self.imgs = sorted(os.listdir(os.path.join(root, \"images\")))\n        self.labels = sorted(os.listdir(os.path.join(root, \"annotations\")))\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n        label_path = os.path.join(self.root, \"annotations\", self.labels[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        with open(label_path) as f:\n            boxes = []\n            labels = []\n            for line in f:\n                annot = line.strip().split()\n                label = int(annot[0])\n                bbox = [float(n) for n in annot[1:]]\n                # Convert to [xmin, ymin, xmax, ymax]\n                boxes.append([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])\n                labels.append(label)\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"id":"k5X-U0-xVS3p","execution":{"iopub.status.busy":"2024-04-02T05:09:21.064998Z","iopub.execute_input":"2024-04-02T05:09:21.065543Z","iopub.status.idle":"2024-04-02T05:09:21.079419Z","shell.execute_reply.started":"2024-04-02T05:09:21.065508Z","shell.execute_reply":"2024-04-02T05:09:21.078323Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Data transformations\ndata_transforms = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ndataset_train = CustomDataset('/kaggle/working/CIS4496_EY/given/AugmentedData/train', transforms=data_transforms)\ndataset_test = CustomDataset('/kaggle/working/CIS4496_EY/given/AugmentedData/test', transforms=data_transforms)\n\n# data_loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\ndata_loader_test = DataLoader(dataset_test, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n\ndata_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)), num_workers=2)","metadata":{"id":"3G2emxNBVS5u","execution":{"iopub.status.busy":"2024-04-02T05:09:52.036837Z","iopub.execute_input":"2024-04-02T05:09:52.037229Z","iopub.status.idle":"2024-04-02T05:09:52.044759Z","shell.execute_reply.started":"2024-04-02T05:09:52.037196Z","shell.execute_reply":"2024-04-02T05:09:52.044013Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n\ndef get_model_instance_segmentation(num_classes):\n    # Load an instance segmentation model pre-trained on COCO\n    weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n    model = fasterrcnn_resnet50_fpn(weights=weights)\n\n    # Get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # Replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model\n\nnum_classes = 4  # Your number of classes here, plus one for the background\nmodel = get_model_instance_segmentation(num_classes)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"perRlFt1VS8E","executionInfo":{"status":"ok","timestamp":1712025872180,"user_tz":240,"elapsed":1070,"user":{"displayName":"Le Mary","userId":"04873967065408965869"}},"outputId":"de95bcfc-43ad-48a1-c53b-ce106c0ae7b2","execution":{"iopub.status.busy":"2024-04-02T05:11:49.821744Z","iopub.execute_input":"2024-04-02T05:11:49.822176Z","iopub.status.idle":"2024-04-02T05:11:50.811222Z","shell.execute_reply.started":"2024-04-02T05:11:49.822145Z","shell.execute_reply":"2024-04-02T05:11:50.810208Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqwIcdpEVS-J","executionInfo":{"status":"ok","timestamp":1712025877032,"user_tz":240,"elapsed":170,"user":{"displayName":"Le Mary","userId":"04873967065408965869"}},"outputId":"1ea0f4e5-4a2e-4056-833e-0ce97073bc48","execution":{"iopub.status.busy":"2024-04-02T05:11:53.564979Z","iopub.execute_input":"2024-04-02T05:11:53.565630Z","iopub.status.idle":"2024-04-02T05:11:53.808613Z","shell.execute_reply.started":"2024-04-02T05:11:53.565599Z","shell.execute_reply":"2024-04-02T05:11:53.807725Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.optim import SGD\nfrom torchvision.ops import batched_nms\n\n\n# Use SGD optimizer\noptimizer = SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    for images, targets in data_loader_train:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n    print(f'Epoch {epoch+1}, Loss: {losses.item()}')","metadata":{"id":"8Sg66vvuVTAP","execution":{"iopub.status.busy":"2024-04-02T05:12:45.931419Z","iopub.execute_input":"2024-04-02T05:12:45.931890Z","iopub.status.idle":"2024-04-02T05:21:42.780231Z","shell.execute_reply.started":"2024-04-02T05:12:45.931833Z","shell.execute_reply":"2024-04-02T05:21:42.779090Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.16892239451408386\nEpoch 2, Loss: 0.3106198012828827\nEpoch 3, Loss: 0.16584014892578125\nEpoch 4, Loss: 0.30579984188079834\nEpoch 5, Loss: 0.26340627670288086\nEpoch 6, Loss: 0.1797814816236496\nEpoch 7, Loss: 0.10230064392089844\nEpoch 8, Loss: 0.09621697664260864\nEpoch 9, Loss: 0.2646562159061432\nEpoch 10, Loss: 0.194094717502594\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.ops import box_iou\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    detection_threshold = 0.5\n    all_precisions = []\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = list(img.to(device) for img in images)\n            outputs = model(images)\n\n            for i, image in enumerate(images):\n                output = outputs[i]\n                target = targets[i]\n\n                output_boxes = output['boxes'].cpu()\n                target_boxes = target['boxes'].cpu()\n\n                iou = box_iou(output_boxes, target_boxes)\n                \n                # Consider detections with IoU > detection_threshold as True Positives (TP)\n                max_iou, _ = torch.max(iou, dim=1)\n                tp = max_iou > detection_threshold\n                num_tp = tp.sum().item()\n                \n                # False Positives (FP) are detections not matching any ground truth box\n                num_fp = (max_iou <= detection_threshold).sum().item()\n                \n                # Precision for this image\n                if num_tp + num_fp > 0:\n                    precision = num_tp / (num_tp + num_fp)\n                else:\n                    precision = 0\n                \n                all_precisions.append(precision)\n\n    # Calculate the mean precision across all test images\n    mean_precision = sum(all_precisions) / len(all_precisions)\n\n    print(f\"Mean Precision: {mean_precision}\")\n\n    return mean_precision\n\n# Call the evaluate function\nevaluate(model, data_loader_test, device)","metadata":{"id":"xOXBU2BHVTCW","execution":{"iopub.status.busy":"2024-04-02T05:27:03.344408Z","iopub.execute_input":"2024-04-02T05:27:03.344786Z","iopub.status.idle":"2024-04-02T05:27:09.300005Z","shell.execute_reply.started":"2024-04-02T05:27:03.344756Z","shell.execute_reply":"2024-04-02T05:27:09.299056Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Mean Precision: 0.011192731992571472\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0.011192731992571472"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"LpSfFXA9VTEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Ke7D6moIVTIm"},"execution_count":null,"outputs":[]}]}